
@online{rosindustrialorg_brief_nodate,
	title = {Brief History},
	url = {https://rosindustrial.org/briefhistory},
	author = {{rosindustrial.org}},
	urldate = {2021-12-09},
}

@online{miura_distributions_2021,
	title = {Distributions - {ROS} Wiki},
	url = {https://wiki.ros.org/Distributions},
	type = {Wiki},
	author = {Miura, Yasuyuki},
	urldate = {2021-12-13},
	date = {2021-05-15},
}

@book{koubaa_robot_2021,
	title = {Robot Operating System ({ROS}): The Complete Reference (Volume 5)},
	volume = {895},
	isbn = {978-3-030-45955-0 978-3-030-45956-7},
	url = {http://link.springer.com/10.1007/978-3-030-45956-7},
	series = {Studies in Computational Intelligence},
	shorttitle = {Robot Operating System ({ROS})},
	publisher = {Springer International Publishing},
	author = {Koubaa, Anis},
	urldate = {2021-09-16},
	date = {2021},
	doi = {10.1007/978-3-030-45956-7},
}

@book{koubaa_robot_2021-1,
	title = {Robot Operating System ({ROS}): The Complete Reference (Volume 6)},
	volume = {962},
	isbn = {978-3-030-75471-6 978-3-030-75472-3},
	url = {https://link.springer.com/10.1007/978-3-030-75472-3},
	series = {Studies in Computational Intelligence},
	shorttitle = {Robot Operating System ({ROS})},
	publisher = {Springer International Publishing},
	author = {Koubaa, Anis},
	urldate = {2021-09-16},
	date = {2021},
	doi = {10.1007/978-3-030-75472-3},
}

@book{koubaa_robot_2020,
	title = {Robot Operating System ({ROS}): The Complete Reference (Volume 4)},
	volume = {831},
	isbn = {978-3-030-20189-0 978-3-030-20190-6},
	url = {http://link.springer.com/10.1007/978-3-030-20190-6},
	series = {Studies in Computational Intelligence},
	shorttitle = {Robot Operating System ({ROS})},
	publisher = {Springer International Publishing},
	author = {Koubaa, Anis},
	urldate = {2021-09-16},
	date = {2020},
	doi = {10.1007/978-3-030-20190-6},
}

@book{koubaa_robot_2017,
	title = {Robot Operating System ({ROS}): The Complete Reference (Volume 2)},
	volume = {707},
	isbn = {978-3-319-54926-2 978-3-319-54927-9},
	url = {http://link.springer.com/10.1007/978-3-319-54927-9},
	series = {Studies in Computational Intelligence},
	shorttitle = {Robot Operating System ({ROS})},
	publisher = {Springer International Publishing},
	author = {Koubaa, Anis},
	urldate = {2021-09-16},
	date = {2017},
	doi = {10.1007/978-3-319-54927-9},
}

@book{koubaa_robot_2016,
	title = {Robot Operating System ({ROS})},
	volume = {625},
	isbn = {978-3-319-26052-5 978-3-319-26054-9},
	url = {http://link.springer.com/10.1007/978-3-319-26054-9},
	series = {Studies in Computational Intelligence},
	publisher = {Springer International Publishing},
	author = {Koubaa, Anis},
	urldate = {2021-09-16},
	date = {2016},
	doi = {10.1007/978-3-319-26054-9},
}

@online{wikirosorg_noeticinstallation_2020,
	title = {noetic/Installation - {ROS} Wiki},
	url = {https://wiki.ros.org/noetic/Installation},
	type = {Wiki},
	author = {{wiki.ros.org}},
	urldate = {2021-12-11},
	date = {2020-12-31},
}

@online{romero_rosconcepts_2014,
	title = {{ROS}/Concepts - {ROS} Wiki},
	url = {https://wiki.ros.org/ROS/Concepts},
	type = {Wiki},
	author = {Romero, Aaron Martinez},
	urldate = {2021-12-12},
	date = {2014-06-21},
}

@online{murray_releases_2021,
	title = {Releases - Ubuntu Wiki},
	url = {https://wiki.ubuntu.com/Releases},
	type = {Wiki},
	author = {Murray, Brian},
	urldate = {2021-12-13},
	date = {2021-10-21},
}

@online{van_eeden_catkinconceptual_overview_2020,
	title = {catkin/conceptual\_overview - {ROS} Wiki},
	url = {http://wiki.ros.org/catkin/conceptual_overview},
	type = {Wiki},
	author = {Van Eeden, Francois},
	urldate = {2021-12-14},
	date = {2020-03-26},
}

@online{thomas_catkinworkspaces_2017,
	title = {catkin/workspaces - {ROS} Wiki},
	url = {http://wiki.ros.org/catkin/workspaces},
	type = {Wiki},
	author = {Thomas, Dirk},
	urldate = {2021-12-14},
	date = {2017-07-07},
}

@article{yurtsever_survey_2020,
	title = {A Survey of Autonomous Driving: Common Practices and Emerging Technologies},
	volume = {8},
	issn = {2169-3536},
	url = {http://arxiv.org/abs/1906.05113},
	doi = {10.1109/ACCESS.2020.2983149},
	shorttitle = {A Survey of Autonomous Driving},
	abstract = {Automated driving systems ({ADSs}) promise a safe, comfortable and efﬁcient driving experience. However, fatalities involving vehicles equipped with {ADSs} are on the rise. The full potential of {ADSs} cannot be realized unless the robustness of state-of-the-art is improved further. This paper discusses unsolved problems and surveys the technical aspect of automated driving. Studies regarding present challenges, highlevel system architectures, emerging methodologies and core functions including localization, mapping, perception, planning, and human machine interfaces, were thoroughly reviewed. Furthermore, many stateof-the-art algorithms were implemented and compared on our own platform in a real-world driving setting. The paper concludes with an overview of available datasets and tools for {ADS} development.},
	pages = {58443--58469},
	journaltitle = {{IEEE} Access},
	shortjournal = {{IEEE} Access},
	author = {Yurtsever, Ekim and Lambert, Jacob and Carballo, Alexander and Takeda, Kazuya},
	urldate = {2021-12-15},
	date = {2020-03-22},
	eprinttype = {arxiv},
	eprint = {1906.05113},
	keywords = {Computer Science - Robotics, Electrical Engineering and Systems Science - Systems and Control},
}

@online{labbe_find_2011,
	title = {Find Object},
	url = {http://introlab.github.io/find-object},
	author = {Labbé, Mathieu},
	urldate = {2021-12-15},
	date = {2011},
}

@article{zou_object_2019,
	title = {Object Detection in 20 Years: A Survey},
	url = {http://arxiv.org/abs/1905.05055},
	shorttitle = {Object Detection in 20 Years},
	abstract = {Object detection, as of one the most fundamental and challenging problems in computer vision, has received great attention in recent years. Its development in the past two decades can be regarded as an epitome of computer vision history. If we think of today's object detection as a technical aesthetics under the power of deep learning, then turning back the clock 20 years we would witness the wisdom of cold weapon era. This paper extensively reviews 400+ papers of object detection in the light of its technical evolution, spanning over a quarter-century's time (from the 1990s to 2019). A number of topics have been covered in this paper, including the milestone detectors in history, detection datasets, metrics, fundamental building blocks of the detection system, speed up techniques, and the recent state of the art detection methods. This paper also reviews some important detection applications, such as pedestrian detection, face detection, text detection, etc, and makes an in-deep analysis of their challenges as well as technical improvements in recent years.},
	journaltitle = {{arXiv}:1905.05055 [cs]},
	author = {Zou, Zhengxia and Shi, Zhenwei and Guo, Yuhong and Ye, Jieping},
	urldate = {2021-12-15},
	date = {2019-05-15},
	eprinttype = {arxiv},
	eprint = {1905.05055},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@incollection{mitchell_image_2010,
	location = {Berlin, Heidelberg},
	title = {Image Key Points},
	isbn = {978-3-642-11216-4},
	url = {https://doi.org/10.1007/978-3-642-11216-4_13},
	abstract = {The subject of this chapter is image key points which we define as a distinctive point in an input image which is invariant to rotation, scale and distortion. In practice, the key points are not perfectly invariant but they are a good approximation. To make our discussion more concrete we shall concentrate on two key point algorithms: {SIFT} and {SURF} and their use in spatial alignment.},
	pages = {163--166},
	booktitle = {Image Fusion: Theories, Techniques and Applications},
	publisher = {Springer},
	author = {Mitchell, H. B.},
	editor = {Mitchell, H. B.},
	urldate = {2021-12-19},
	date = {2010},
	doi = {10.1007/978-3-642-11216-4_13},
	keywords = {Complex Wavelet, Hyperspectral Image, Input Image, Interest Point, Sift Descriptor},
}

@book{joseph_mastering_2018,
	location = {Birmingham Mumbai},
	edition = {Second edition},
	title = {Mastering {ROS} for robotics programming: design, build, and simulate complex robots using the Robot Operating System},
	isbn = {978-1-78847-895-3},
	shorttitle = {Mastering {ROS} for robotics programming},
	pagetotal = {559},
	publisher = {Packt},
	author = {Joseph, Lentin and Cacace, Jonathan},
	date = {2018},
}

@online{mohan_6_2020,
	title = {6 Different Types of Object Detection Algorithms in Nutshell},
	url = {https://machinelearningknowledge.ai/different-types-of-object-detection-algorithms/},
	abstract = {In this article we will cover some popular different types of object detection algorithms along with their advantages and limitations.},
	author = {Mohan, Sachin},
	urldate = {2021-12-19},
	date = {2020-06-30},
}

@article{youzi_review_2020,
	title = {A review of object detection based on deep learning},
	volume = {79},
	doi = {10.1007/s11042-020-08976-6},
	abstract = {With the rapid development of deep learning techniques, deep convolutional neural networks ({DCNNs}) have become more important for object detection. Compared with traditional handcrafted feature-based methods, the deep learning-based object detection methods can learn both low-level and high-level image features. The image features learned through deep learning techniques are more representative than the handcrafted features. Therefore, this review paper focuses on the object detection algorithms based on deep convolutional neural networks, while the traditional object detection algorithms will be simply introduced as well. Through the review and analysis of deep learning-based object detection techniques in recent years, this work includes the following parts: backbone networks, loss functions and training strategies, classical object detection architectures, complex problems, datasets and evaluation metrics, applications and future development directions. We hope this review paper will be helpful for researchers in the field of object detection.},
	journaltitle = {Multimedia Tools and Applications},
	shortjournal = {Multimedia Tools and Applications},
	author = {Youzi, Xiao and Tian, Zhiqiang and Yu, Jiachen and Zhang, Yinshu and Liu, Shuai and Du, Shaoyi and Lan, Xuguang},
	date = {2020-09-01},
}

@software{wiedemeyer_iai_2021,
	title = {{IAI} Kinect2},
	url = {https://github.com/code-iai/iai_kinect2/blob/0e2c5f63134a076606bb79963406e1d47f2da651/kinect2_calibration/README.md},
	abstract = {Tools for using the Kinect One (Kinect v2) in {ROS}},
	publisher = {Institute for Artificial Intelligence - University of Bremen},
	author = {Wiedemeyer, Thiemo},
	urldate = {2021-12-20},
	date = {2021-12-16},
}

@software{wiedemeyer_iai_kinect2kinect2_bridgedata_2015,
	title = {iai\_kinect2/kinect2\_bridge/data},
	url = {https://github.com/code-iai/iai_kinect2},
	abstract = {Tools for using the Kinect One (Kinect v2) in {ROS}. Contribute to code-iai/iai\_kinect2 development by creating an account on {GitHub}.},
	publisher = {Institute for Artificial Intelligence - University of Bremen},
	author = {Wiedemeyer, Thiemo},
	urldate = {2021-12-20},
	date = {2015-05-18},
}

@software{redmon_darknet_2021,
	title = {Darknet},
	url = {https://github.com/pjreddie/darknet},
	abstract = {Convolutional Neural Networks},
	author = {Redmon, Joseph},
	urldate = {2021-12-20},
	date = {2021-12-20},
}

@software{intelligentroboticslabs_darknet_ros_3d_2021,
	title = {darknet\_ros\_3d},
	url = {https://github.com/IntelligentRoboticsLabs/gb_visual_detection_3d},
	publisher = {Intelligent Robotics Labs},
	author = {{IntelligentRoboticsLabs}},
	urldate = {2021-12-20},
	date = {2021-12-16},
}

@online{opencvorg_camera_2019,
	title = {Camera Calibration and 3D Reconstruction — {OpenCV} 2.4.13.7 documentation},
	url = {https://docs.opencv.org/2.4/modules/calib3d/doc/camera_calibration_and_3d_reconstruction.html},
	author = {{opencv.org}},
	urldate = {2021-12-27},
	date = {2019-12-31},
	langid = {english},
}

@article{sung_real-time_2019,
	title = {Real-Time Augmented Reality Physics Simulator for Education},
	volume = {9},
	url = {https://www.researchgate.net/figure/Microsoft-Kinect-V2s-Hardware-Structure-Information-Resolution-Frame-per-Second-FPS_fig5_336061295},
	doi = {10.3390/app9194019},
	abstract = {Physics education applications using augmented reality technology, which has been developed extensively in recent years, have a lot of restrictions in terms of performance and accuracy. The purpose of our research is to develop a real-time simulation system for physics education that is based on parallel processing. In this paper, we present a video see-through {AR} (Augmented Reality) system that includes an environment recognizer using a depth image of Microsoft’s Kinect V2 and a real-time soft body simulator based on parallel processing using {GPU} (Graphic Processing Unit). Soft body simulation can provide more realistic simulation results than rigid body simulation, so it can be more effective in systems for physics education. We have designed and implemented a system that provides the physical deformation and movement of 3D volumetric objects, and uses them in education. To verify the usefulness of the proposed system, we conducted a questionnaire survey of 10 students majoring in physics education. As a result of the questionnaire survey, 93\% of respondents answered that they would like to use it for education. We plan to use the stand-alone {AR} device including one or more cameras to improve the system in the future.},
	pages = {4019},
	journaltitle = {Applied Sciences},
	shortjournal = {Applied Sciences},
	author = {Sung, Nak-Jun and {Ma} and Choi, Chil-Sung and Hong, Gwi-Ryung},
	date = {2019-09-25},
}

@software{tzutalin_labelimg_2015,
	title = {{LabelImg}},
	url = {https://github.com/tzutalin/labelImg},
	abstract = {🖍️ {LabelImg} is a graphical image annotation tool and label object bounding boxes in images},
	author = {{Tzutalin}},
	urldate = {2022-01-04},
	date = {2015},
	keywords = {annotations, deep-learning, detection, image-classification, imagenet, python2, python3, recognition, tools},
}

@online{scott_ros_2021,
	title = {{ROS} News for the Week of 5/10/2021 - General},
	url = {https://discourse.ros.org/t/ros-news-for-the-week-of-5-10-2021/20399},
	abstract = {{ROS} News for the Week of 5/10/2021 Arduino Portenta for Machine Control – Should run micro-{ROS} Obituary for Kinetic Kame How did Kinetic impact your life? What robots did you build with it? Check out the thread above or this Twitter thread and let us know what robots you built with Kinetic. Events 5/18/2021 {ROS} 2 Industrial Training 5/19/2021 Online Robotics Job Fair by {SVRobo} 5/19/2021 Mass Robotics Career Fair 5/22/2021 {DIY} Robocars and {BBQ} Return to Circuit Launch 5/31/2021 {ICRA}...},
	type = {Blog},
	author = {Scott, Katherine},
	urldate = {2022-01-17},
	date = {2021-05-14},
}

@online{bandyopadhyay_yolo_2021,
	title = {{YOLO}: Real-Time Object Detection Explained},
	url = {https://www.v7labs.com/blog/yolo-object-detection},
	shorttitle = {{YOLO}},
	abstract = {What is {YOLO} and how does it work? Learn about different {YOLO} versions and start training your own {YOLO} object detection models.},
	type = {Blog},
	author = {Bandyopadhyay, Hmrishav},
	urldate = {2022-01-11},
	date = {2021-09-09},
}

@misc{bjelonic_yolo_2016,
	title = {{YOLO} {ROS}: Real-Time Object Detection for {ROS}},
	url = {https://github.com/leggedrobotics/darknet_ros},
	author = {Bjelonic, Marko},
	date = {2016},
}

@online{joseph_redmon_i_2020,
	title = {I stopped doing {CV} research},
	url = {https://twitter.com/pjreddie/status/1230524770350817280},
	type = {Tweet},
	author = {{Joseph Redmon}},
	urldate = {2021-12-20},
	date = {2020-02-20},
}

@article{bochkovskiy_yolov4_2020,
	title = {{YOLOv}4: Optimal Speed and Accuracy of Object Detection},
	url = {http://arxiv.org/abs/2004.10934},
	shorttitle = {{YOLOv}4},
	abstract = {There are a huge number of features which are said to improve Convolutional Neural Network ({CNN}) accuracy. Practical testing of combinations of such features on large datasets, and theoretical justification of the result, is required. Some features operate on certain models exclusively and for certain problems exclusively, or only for small-scale datasets; while some features, such as batch-normalization and residual-connections, are applicable to the majority of models, tasks, and datasets. We assume that such universal features include Weighted-Residual-Connections ({WRC}), Cross-Stage-Partial-connections ({CSP}), Cross mini-Batch Normalization ({CmBN}), Self-adversarial-training ({SAT}) and Mish-activation. We use new features: {WRC}, {CSP}, {CmBN}, {SAT}, Mish activation, Mosaic data augmentation, {CmBN}, {DropBlock} regularization, and {CIoU} loss, and combine some of them to achieve state-of-the-art results: 43.5\% {AP} (65.7\% {AP}50) for the {MS} {COCO} dataset at a realtime speed of {\textbackslash}textasciitilde65 {FPS} on Tesla V100. Source code is at https://github.com/{AlexeyAB}/darknet},
	journaltitle = {{arXiv}:2004.10934 [cs, eess]},
	author = {Bochkovskiy, Alexey and Wang, Chien-Yao and Liao, Hong-Yuan Mark},
	urldate = {2021-12-20},
	date = {2020-04-22},
	eprinttype = {arxiv},
	eprint = {2004.10934},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Electrical Engineering and Systems Science - Image and Video Processing},
}

@article{redmon_you_2016,
	title = {You Only Look Once: Unified, Real-Time Object Detection},
	url = {http://arxiv.org/abs/1506.02640},
	shorttitle = {You Only Look Once},
	abstract = {We present {YOLO}, a new approach to object detection. Prior work on object detection repurposes classifiers to perform detection. Instead, we frame object detection as a regression problem to spatially separated bounding boxes and associated class probabilities. A single neural network predicts bounding boxes and class probabilities directly from full images in one evaluation. Since the whole detection pipeline is a single network, it can be optimized end-to-end directly on detection performance. Our unified architecture is extremely fast. Our base {YOLO} model processes images in real-time at 45 frames per second. A smaller version of the network, Fast {YOLO}, processes an astounding 155 frames per second while still achieving double the {mAP} of other real-time detectors. Compared to state-of-the-art detection systems, {YOLO} makes more localization errors but is far less likely to predict false detections where nothing exists. Finally, {YOLO} learns very general representations of objects. It outperforms all other detection methods, including {DPM} and R-{CNN}, by a wide margin when generalizing from natural images to artwork on both the Picasso Dataset and the People-Art Dataset.},
	journaltitle = {{arXiv}:1506.02640 [cs]},
	author = {Redmon, Joseph and Divvala, Santosh and Girshick, Ross and Farhadi, Ali},
	urldate = {2021-12-20},
	date = {2016-05-09},
	eprinttype = {arxiv},
	eprint = {1506.02640},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@online{clearpath_robotics_intro_2015,
	title = {Intro to {ROS} — {ROS} Tutorials 0.5.2 documentation},
	url = {https://www.clearpathrobotics.com/assets/guides/kinetic/ros/Intro%20to%20the%20Robot%20Operating%20System.html},
	author = {{Clearpath Robotics}},
	urldate = {2021-12-19},
	date = {2015},
}

@article{cortes_support-vector_1995,
	title = {Support-vector networks},
	volume = {20},
	issn = {0885-6125, 1573-0565},
	url = {http://link.springer.com/10.1007/BF00994018},
	doi = {10.1007/BF00994018},
	abstract = {The support-vector network is a new learning machine for two-group classification problems. The machine conceptually implements the following idea: input vectors are non-linearly mapped to a very highdimension feature space. In this feature space a linear decision surface is constructed. Special properties of the decision surface ensures high generalization ability of the learning machine. The idea behind the support-vector network was previously implemented for the restricted case where the training data can be separated without errors. We here extend this result to non-separable training data.},
	pages = {273--297},
	number = {3},
	journaltitle = {Machine Learning},
	shortjournal = {Mach Learn},
	author = {Cortes, Corinna and Vapnik, Vladimir},
	urldate = {2021-12-19},
	date = {1995-09},
}

@article{lowe_distinctive_2004,
	title = {Distinctive Image Features from Scale-Invariant Keypoints},
	volume = {60},
	issn = {1573-1405},
	url = {https://doi.org/10.1023/B:VISI.0000029664.99615.94},
	doi = {10.1023/B:VISI.0000029664.99615.94},
	abstract = {This paper presents a method for extracting distinctive invariant features from images that can be used to perform reliable matching between different views of an object or scene. The features are invariant to image scale and rotation, and are shown to provide robust matching across a substantial range of affine distortion, change in 3D viewpoint, addition of noise, and change in illumination. The features are highly distinctive, in the sense that a single feature can be correctly matched with high probability against a large database of features from many images. This paper also describes an approach to using these features for object recognition. The recognition proceeds by matching individual features to a database of features from known objects using a fast nearest-neighbor algorithm, followed by a Hough transform to identify clusters belonging to a single object, and finally performing verification through least-squares solution for consistent pose parameters. This approach to recognition can robustly identify objects among clutter and occlusion while achieving near real-time performance.},
	pages = {91--110},
	number = {2},
	journaltitle = {International Journal of Computer Vision},
	shortjournal = {International Journal of Computer Vision},
	author = {Lowe, David G.},
	date = {2004-11-01},
	note = {Number: 2},
}

@inproceedings{bay_surf_2006,
	title = {{SURF}: Speeded Up Robust Features},
	isbn = {978-3-540-33833-8},
	abstract = {In this paper, we present a novel scale- and rotation-invariant interest point detector and descriptor, coined {SURF} (Speeded Up Robust Features). It approximates or even outperforms previously proposed schemes with respect to repeatability, distinctiveness, and robustness, yet can be computed and compared much faster.},
	pages = {404--417},
	booktitle = {Computer Vision – {ECCV} 2006},
	publisher = {Springer Berlin Heidelberg},
	author = {Bay, Herbert and Tuytelaars, Tinne and Van Gool, Luc},
	editor = {Leonardis, Aleš and Bischof, Horst and Pinz, Axel},
	date = {2006},
	note = {Veranstaltungsort: Berlin, Heidelberg},
}

@inproceedings{bieller_world_2021,
	title = {World Robotics 2021},
	url = {https://ifr.org/downloads/press2018/2021_10_28_WR_PK_Presentation_long_version.pdf},
	pages = {43},
	author = {Bieller, Dr. Susanne and Müller, Dr. Christopher and Kraus, Dr. Werner and Guerry, Milton},
	date = {2021-10-28},
}

@online{loretz_rosdep_2020,
	title = {rosdep - {ROS} Wiki},
	url = {https://wiki.ros.org/rosdep},
	type = {Wiki},
	author = {Loretz, Shane},
	urldate = {2021-12-14},
	date = {2020-05-20},
}

@online{staples_support_2020,
	title = {Support - {ROS} Wiki},
	url = {https://wiki.ros.org/Support},
	type = {Wiki},
	author = {Staples, Gabriel},
	urldate = {2021-12-13},
	date = {2020-11-06},
}

@online{dattalo_rosintroduction_2018,
	title = {{ROS}/Introduction - {ROS} Wiki},
	url = {https://wiki.ros.org/ROS/Introduction},
	type = {Wiki},
	author = {Dattalo, Amanda},
	urldate = {2021-12-11},
	date = {2018-08-08},
}

@online{rosorg_roshome_2021,
	title = {{ROS}/Home},
	url = {https://www.ros.org/},
	author = {{ros.org}},
	urldate = {2021-12-11},
	date = {2021},
}

@online{open_robotics_distributions_2021,
	title = {Distributions — {ROS} 2 Documentation: Foxy documentation},
	url = {https://docs.ros.org/en/foxy/Releases.html},
	type = {Wiki},
	author = {{Open Robotics}},
	urldate = {2021-12-13},
	date = {2021},
}

@incollection{arai_deep_2020,
	location = {Cham},
	title = {Deep Learning vs. Traditional Computer Vision},
	volume = {943},
	isbn = {978-3-030-17794-2 978-3-030-17795-9},
	url = {http://link.springer.com/10.1007/978-3-030-17795-9_10},
	abstract = {Deep Learning has pushed the limits of what was possible in the domain of Digital Image Processing. However, that is not to say that the traditional computer vision techniques which had been undergoing progressive development in years prior to the rise of {DL} have become obsolete. This paper will analyse the benefits and drawbacks of each approach. The aim of this paper is to promote a discussion on whether knowledge of classical computer vision techniques should be maintained. The paper will also explore how the two sides of computer vision can be combined. Several recent hybrid methodologies are reviewed which have demonstrated the ability to improve computer vision performance and to tackle problems not suited to Deep Learning. For example, combining traditional computer vision techniques with Deep Learning has been popular in emerging domains such as Panoramic Vision and 3D vision for which Deep Learning models have not yet been fully optimised.},
	pages = {128--144},
	booktitle = {Advances in Computer Vision},
	publisher = {Springer International Publishing},
	author = {O’Mahony, Niall and Campbell, Sean and Carvalho, Anderson and Harapanahalli, Suman and Hernandez, Gustavo Velasco and Krpalkova, Lenka and Riordan, Daniel and Walsh, Joseph},
	editor = {Arai, Kohei and Kapoor, Supriya},
	urldate = {2022-02-04},
	date = {2020},
	langid = {english},
	doi = {10.1007/978-3-030-17795-9_10},
	note = {Titel der Serie: Advances in Intelligent Systems and Computing},
	file = {O’Mahony et al. - 2020 - Deep Learning vs. Traditional Computer Vision.pdf:/home/david/snap/zotero-snap/common/Zotero/storage/3WRU822K/O’Mahony et al. - 2020 - Deep Learning vs. Traditional Computer Vision.pdf:application/pdf},
}

@book{bonaccorso_machine_2018,
	edition = {2},
	title = {Machine Learning Algorithms},
	isbn = {978-1-78934-799-9},
	url = {https://www.packtpub.com/product/machine-learning-algorithms-second-edition/9781789347999},
	series = {Machine Learning Algorithms},
	abstract = {An easy-to-follow, step-by-step guide for getting to grips with the real-world application of machine learning algorithms},
	publisher = {Packt Publishing},
	author = {Bonaccorso, Giuseppe},
	urldate = {2022-02-04},
	date = {2018},
	langid = {english},
	file = {Snapshot:/home/david/snap/zotero-snap/common/Zotero/storage/CDYV75WB/9781789347999.html:text/html},
}

@inproceedings{o_mahony_real-time_2017,
	title = {Real-time monitoring of powder blend composition using near infrared spectroscopy},
	doi = {10.1109/ICSensT.2017.8304431},
	pages = {1--6},
	author = {O' Mahony, Niall and Murphy, Trevor and Panduru, Krishna and Riordan, Daniel and Walsh, Joseph},
	date = {2017-12-01},
	file = {Full Text PDF:/home/david/snap/zotero-snap/common/Zotero/storage/DMET8AUP/O' Mahony et al. - 2017 - Real-time monitoring of powder blend composition u.pdf:application/pdf},
}

@inproceedings{o_mahony_adaptive_2016,
	title = {Adaptive process control and sensor fusion for process analytical technology},
	doi = {10.1109/ISSC.2016.7528449},
	abstract = {Increased globalisation and competition are drivers for process analytical technologies ({PAT}) that enable seamless process control, greater flexibility and cost efficiency in the process industries. This research aims to introduce an integrated process control approach, embedding novel sensors for monitoring in real time the critical control parameters of key processes in the minerals, ceramics, non-ferrous metals, and chemical process industries. The paper will discuss smart sensors, data fusion and process modelling and control in industrial applications with an emphasis on solutions enabling the real-time data analytics of sensor measurements that {PAT} demands.},
	eventtitle = {2016 27th Irish Signals and Systems Conference ({ISSC})},
	pages = {1--6},
	booktitle = {2016 27th Irish Signals and Systems Conference ({ISSC})},
	author = {O' Mahony, Niall and Murphy, Trevor and Panduru, Krishna and Riordan, Daniel and Walsh, Joseph},
	date = {2016-06},
	keywords = {Biological neural networks, data fusion, Data integration, Industries, Intelligent sensors, Machine learning, Neurons, predictive control, process analytical technology, Process control, Sensor fusion},
	file = {IEEE Xplore Full Text PDF:/home/david/snap/zotero-snap/common/Zotero/storage/XPVHUCUS/O' Mahony et al. - 2016 - Adaptive process control and sensor fusion for pro.pdf:application/pdf;IEEE Xplore Abstract Record:/home/david/snap/zotero-snap/common/Zotero/storage/TL4NSYFW/7528449.html:text/html},
}

@software{intel_corporation_ros_2022,
	title = {{ROS} Wrapper for Intel® {RealSense}™ Devices},
	rights = {Apache-2.0},
	url = {https://github.com/IntelRealSense/realsense-ros},
	abstract = {Intel(R) {RealSense}({TM}) {ROS} Wrapper for D400 series, {SR}300 Camera and T265 Tracking Module},
	publisher = {Intel® {RealSense}™},
	author = {{Intel Corporation}},
	urldate = {2022-02-08},
	date = {2022-02-07},
	note = {Erstveröffentlichung: 23.02.2016},
}

@software{intel_corporation_intelrealsenselibrealsense_2022,
	title = {{IntelRealSense}/librealsense},
	url = {https://github.com/IntelRealSense/librealsense},
	abstract = {Intel® {RealSense}™ {SDK}},
	publisher = {Intel® {RealSense}™},
	author = {{Intel Corporation}},
	urldate = {2022-02-08},
	date = {2022-02-08},
	note = {Erstveröffentlichung: 17.11.2015},
	keywords = {camera-api, computer-vision, developer-kits, hardware, library, librealsense, sdk},
}

@software{xiang_libfreenect2_2016,
	title = {libfreenect2: Release 0.2},
	url = {https://zenodo.org/record/50641},
	shorttitle = {libfreenect2},
	abstract = {This release provides performance improvements in {JPEG} and depth decoding with various implementations which enable hardware acceleration, including {CUDA}, {VA}-{API}, {VideoToolbox}, and Tegra. See also this performance benchmark. {OpenNI}2 integration is included in this release. Various internal performance improvements, error handling, and bug fixes are also included. For programmers, the {API} is expanded to provide Freenect2Device::{startStreams}() (selectively start {RGB} or depth stream), Registration::{undistortDepth}() (rectification only without registration), and Registration::{getPointXYZ}() (3-D points without color).},
	publisher = {Zenodo},
	author = {Xiang, Lingzhu and Echtler, Florian and Kerl, Christian and Wiedemeyer, Thiemo and Lars and hanyazou and Gordon, Ryan and Facioni, Francisco and laborer2008 and Wareham, Rich and Goldhoorn, Matthias and alberth and gaborpapp and Fuchs, Steffen and jmtatsch and Blake, Joshua and Federico and Jungkurth, Henning and Mingze, Yuan and vinouz and Coleman, Dave and Burns, Brendan and Rawat, Rahul and Mokhov, Serguei and Reynolds, Paul and Viau, P. E. and Fraissinet-Tachet, Matthieu and Ludique and Billingham, James and Alistair},
	urldate = {2022-02-08},
	date = {2016-04-28},
	doi = {10.5281/zenodo.50641},
	file = {Zenodo Snapshot:/home/david/snap/zotero-snap/common/Zotero/storage/CFTU6XB6/50641.html:text/html},
}

@online{occipital_openni_2022,
	title = {{OpenNI} Programmers Guide},
	url = {https://s3.amazonaws.com/com.occipital.openni/OpenNI_Programmers_Guide.pdf},
	titleaddon = {{OpenNI} 2 {SDK} Binaries \& Docs},
	author = {{Occipital}},
	urldate = {2022-02-08},
	date = {2022},
	langid = {english},
	file = {OpenNI_Programmers_Guide.pdf:/home/david/snap/zotero-snap/common/Zotero/storage/9NTK4A99/OpenNI_Programmers_Guide.pdf:application/pdf},
}

@article{lowe_distinctive_2004-1,
	title = {Distinctive Image Features from Scale-Invariant Keypoints},
	volume = {60},
	issn = {1573-1405},
	url = {https://doi.org/10.1023/B:VISI.0000029664.99615.94},
	doi = {10.1023/B:VISI.0000029664.99615.94},
	abstract = {This paper presents a method for extracting distinctive invariant features from images that can be used to perform reliable matching between different views of an object or scene. The features are invariant to image scale and rotation, and are shown to provide robust matching across a substantial range of affine distortion, change in 3D viewpoint, addition of noise, and change in illumination. The features are highly distinctive, in the sense that a single feature can be correctly matched with high probability against a large database of features from many images. This paper also describes an approach to using these features for object recognition. The recognition proceeds by matching individual features to a database of features from known objects using a fast nearest-neighbor algorithm, followed by a Hough transform to identify clusters belonging to a single object, and finally performing verification through least-squares solution for consistent pose parameters. This approach to recognition can robustly identify objects among clutter and occlusion while achieving near real-time performance.},
	pages = {91--110},
	number = {2},
	journaltitle = {International Journal of Computer Vision},
	shortjournal = {International Journal of Computer Vision},
	author = {Lowe, David G.},
	date = {2004-11-01},
}

@misc{steinbeck_entwicklung_2022,
	title = {Entwicklung einer {KI}-basierten Robotersteuerung mit Industrial {ROS} - Generierung von Handhabungssequenzen für mechanische Konstruktionen nach gegebenen Bauplänen},
	author = {Steinbeck, Dennis},
	date = {2022-02-27},
	langid = {german},
}

@software{the_ai_guy_yologeneratetrainingfile_2021,
	title = {{YoloGenerateTrainingFile}},
	url = {https://github.com/theAIGuysCode/YoloGenerateTrainingFile/blob/2bcb9b1ba7e75061ce1c267a48860c67cb229b62/generate_train.py},
	abstract = {Script to generate train.txt for custom yolov3 object detection.},
	author = {{The AI Guy}},
	urldate = {2022-02-14},
	date = {2021-12-17},
	note = {Erstveröffentlichung: 08.01.2020},
}

@online{intel_corporation_depth_2022,
	title = {Depth Camera D435},
	url = {https://www.intelrealsense.com/depth-camera-d435/},
	abstract = {Depth Camera D435 offers the widest field of view of all our cameras, global shutter on the depth sensor that is ideal for fast moving applications.},
	titleaddon = {Intel® {RealSense}™ Depth and Tracking Cameras},
	author = {{Intel Corporation}},
	urldate = {2022-02-14},
	date = {2022},
	langid = {american},
	file = {Snapshot:/home/david/snap/zotero-snap/common/Zotero/storage/Z6X2I7VT/depth-camera-d435.html:text/html},
}

@online{grunnet-jepsen_intel_2021,
	title = {Intel® {RealSense}™ Self-Calibration for D400 Series Depth Cameras},
	url = {https://dev.intelrealsense.com/docs/self-calibration-for-depth-cameras},
	titleaddon = {Intel® {RealSense}™ Developer Documentation},
	author = {Grunnet-Jepsen, Andreas and Sweetser, John and Khuong, Tri and Tong, Dave and Mulla, Ofir},
	urldate = {2022-02-15},
	date = {2021},
	langid = {english},
	note = {Revision 2.1},
	file = {Snapshot:/home/david/snap/zotero-snap/common/Zotero/storage/HWLEJNVG/self-calibration-for-depth-cameras.html:text/html},
}

@incollection{chen_comparison_2017,
	location = {Cham},
	title = {Comparison of Kinect V1 and V2 Depth Images in Terms of Accuracy and Precision},
	volume = {10117},
	isbn = {978-3-319-54426-7 978-3-319-54427-4},
	url = {http://link.springer.com/10.1007/978-3-319-54427-4_3},
	abstract = {{RGB}-D cameras like the Microsoft Kinect had a huge impact on recent research in Computer Vision as well as Robotics. With the release of the Kinect v2 a new promising device is available, which will – most probably – be used in many future research. In this paper, we present a systematic comparison of the Kinect v1 and Kinect v2. We investigate the accuracy and precision of the devices for their usage in the context of 3D reconstruction, {SLAM} or visual odometry. For each device we rigorously ﬁgure out and quantify inﬂuencing factors on the depth images like temperature, the distance of the camera or the scene color. Furthermore, we demonstrate errors like ﬂying pixels and multipath interference. Our insights build the basis for incorporating or modeling the errors of the devices in follow-up algorithms for diverse applications.},
	pages = {34--45},
	booktitle = {Computer Vision – {ACCV} 2016 Workshops},
	publisher = {Springer International Publishing},
	author = {Wasenmüller, Oliver and Stricker, Didier},
	editor = {Chen, Chu-Song and Lu, Jiwen and Ma, Kai-Kuang},
	urldate = {2022-02-15},
	date = {2017},
	langid = {english},
	doi = {10.1007/978-3-319-54427-4_3},
	note = {Titel der Serie: Lecture Notes in Computer Science},
	file = {Wasenmüller und Stricker - 2017 - Comparison of Kinect V1 and V2 Depth Images in Ter.pdf:/home/david/snap/zotero-snap/common/Zotero/storage/PH5XW7MF/Wasenmüller und Stricker - 2017 - Comparison of Kinect V1 and V2 Depth Images in Ter.pdf:application/pdf},
}

@article{noonan_repurposing_2015,
	title = {Repurposing the Microsoft Kinect for Windows v2 for external head motion tracking for brain {PET}},
	volume = {60},
	issn = {0031-9155, 1361-6560},
	url = {https://iopscience.iop.org/article/10.1088/0031-9155/60/22/8753},
	doi = {10.1088/0031-9155/60/22/8753},
	abstract = {Medical imaging systems such as those used in positron emission tomography ({PET}) are capable of spatial resolutions that enable the imaging of small, functionally important brain structures. However, the quality of data from {PET} brain studies is often limited by subject motion during acquisition. This is particularly challenging for patients with neurological disorders or with dynamic research studies that can last 90 min or more. Restraining head movement during the scan does not eliminate motion entirely and can be unpleasant for the subject. Head motion can be detected and measured using a variety of techniques that either use the {PET} data itself or an external tracking system. Advances in computer vision arising from the video gaming industry could offer significant benefits when re-purposed for medical applications. A method for measuring rigid body type head motion using the Microsoft Kinect v2 is described with results presenting ⩽  0.5 mm spatial accuracy. Motion data is measured in real-time at 30 Hz using the {KinectFusion} algorithm. Non-rigid motion is detected using the residual alignment energy data of the {KinectFusion} algorithm allowing for unreliable motion to be discarded. Motion data is aligned to {PET} listmode data using injected pulse sequences into the {PET}/{CT} gantry allowing for correction of rigid body motion. Pilot data from a clinical dynamic {PET}/{CT} examination is shown.},
	pages = {8753--8766},
	number = {22},
	journaltitle = {Physics in Medicine and Biology},
	shortjournal = {Phys. Med. Biol.},
	author = {Noonan, P J and Howard, J and Hallett, W A and Gunn, R N},
	urldate = {2022-02-15},
	date = {2015-11-21},
	langid = {english},
	file = {Noonan et al. - 2015 - Repurposing the Microsoft Kinect for Windows v2 fo.pdf:/home/david/snap/zotero-snap/common/Zotero/storage/YQFL28AP/Noonan et al. - 2015 - Repurposing the Microsoft Kinect for Windows v2 fo.pdf:application/pdf},
}

@software{ulisse_perusin_ros-visualizationrviz_2022,
	title = {ros-visualization/rviz},
	url = {https://github.com/ros-visualization/rviz},
	abstract = {{ROS} 3D Robot Visualizer},
	publisher = {ros-visualization},
	author = {{Ulisse Perusin} and {Steven Garrity} and {Lapo Calamandrei} and {Ryan Collier} and {Rodney Dawes} and {Andreas Nilsson} and {Tuomas Kuosmanen} and {Garrett LeSage} and {Jakub Steiner} and {David Gossow} and {Chad Rockey} and {Kei Okada} and {Julius Kammerl} and {Acorn Pooley} and {Rein Appeldoorn} and {Robert Haschke}},
	urldate = {2022-02-15},
	date = {2022-02-14},
	note = {Erstveröffentlichung: 18.09.2012},
}

@online{ioan_a_sucan_moveit_nodate,
	title = {{MoveIt} Motion Planning Framework},
	url = {https://moveit.ros.org/},
	abstract = {{MoveIt} is the most widely used software for manipulation and has been used on over 150 robots. It is released under the terms of the {BSD} license, and thus free for industrial, commercial, and research use.

By incorporating the latest advances in motion planning, manipulation, 3D perception, kinematics, control and navigation, {MoveIt} is state of the art software for mobile manipulation.},
	titleaddon = {{MoveIt}},
	author = {{Ioan A. Sucan} and {Sachin Chitta}},
	urldate = {2022-02-15},
	langid = {english},
	file = {MoveIt Motion Planning Framework:/home/david/snap/zotero-snap/common/Zotero/storage/Q4ZEV9TT/moveit.ros.org.html:text/html},
}

@article{tabb_solving_2017,
	title = {Solving the Robot-World Hand-Eye(s) Calibration Problem with Iterative Methods},
	volume = {28},
	issn = {0932-8092, 1432-1769},
	url = {http://arxiv.org/abs/1907.12425},
	doi = {10.1007/s00138-017-0841-7},
	abstract = {Robot-world, hand-eye calibration is the problem of determining the transformation between the robot end-effector and a camera, as well as the transformation between the robot base and the world coordinate system. This relationship has been modeled as \${\textbackslash}mathbf\{{AX}\}={\textbackslash}mathbf\{{ZB}\}\$, where \${\textbackslash}mathbf\{X\}\$ and \${\textbackslash}mathbf\{Z\}\$ are unknown homogeneous transformation matrices. The successful execution of many robot manipulation tasks depends on determining these matrices accurately, and we are particularly interested in the use of calibration for use in vision tasks. In this work, we describe a collection of methods consisting of two cost function classes, three different parameterizations of rotation components, and separable versus simultaneous formulations. We explore the behavior of this collection of methods on real datasets and simulated datasets, and compare to seven other state-of-the-art methods. Our collection of methods return greater accuracy on many metrics as compared to the state-of-the-art. The collection of methods is extended to the problem of robot-world hand-multiple-eye calibration, and results are shown with two and three cameras mounted on the same robot.},
	pages = {569--590},
	number = {5},
	journaltitle = {Machine Vision and Applications},
	shortjournal = {Machine Vision and Applications},
	author = {Tabb, Amy and Yousef, Khalil M. Ahmad},
	urldate = {2022-02-15},
	date = {2017-08},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1907.12425},
	keywords = {Computer Science - Robotics, Computer Science - Computer Vision and Pattern Recognition},
	file = {Tabb und Yousef - 2017 - Solving the Robot-World Hand-Eye(s) Calibration Pr.pdf:/home/david/snap/zotero-snap/common/Zotero/storage/4U48VIGC/Tabb und Yousef - 2017 - Solving the Robot-World Hand-Eye(s) Calibration Pr.pdf:application/pdf},
}

@software{rauch_moveit_2022,
	title = {{MoveIt} Calibration},
	rights = {{BSD}-3-Clause},
	url = {https://github.com/ros-planning/moveit_calibration},
	abstract = {Hand-eye calibration tools for robot arms.},
	publisher = {{ROS} Planning},
	author = {Rauch, Christian and Haschke, Robert and Weaver, Tyler and {RoboticsYY} and Coleman, Dave and Magnago, Valerio and Stechschulte, John and Brahmbhatt, Samarth and {Jakubach} and Ferguson, Michael and Brijacak, Inka},
	urldate = {2022-02-15},
	date = {2022-02-11},
	note = {Erstveröffentlichung: 02.06.2020},
}

@online{stechschulte_hand-eye_2022,
	title = {Hand-Eye Calibration — moveit\_tutorials Noetic documentation},
	url = {https://ros-planning.github.io/moveit_tutorials/doc/hand_eye_calibration/hand_eye_calibration_tutorial.html},
	author = {Stechschulte, John},
	urldate = {2022-02-15},
	date = {2022},
	file = {Hand-Eye Calibration — moveit_tutorials Noetic documentation:/home/david/snap/zotero-snap/common/Zotero/storage/R2VNZTQA/hand_eye_calibration_tutorial.html:text/html},
}

@article{quigley_ros_2009,
	title = {{ROS}: an open-source Robot Operating System},
	abstract = {This paper gives an overview of {ROS}, an opensource robot operating system. {ROS} is not an operating system in the traditional sense of process management and scheduling; rather, it provides a structured communications layer above the host operating systems of a heterogenous compute cluster. In this paper, we discuss how {ROS} relates to existing robot software frameworks, and brieﬂy overview some of the available application software which uses {ROS}.},
	pages = {6},
	number = {3},
	author = {Quigley, Morgan and Gerkey, Brian and Conley, Ken and Faust, Josh and Foote, Tully and Leibs, Jeremy and Berger, Eric and Wheeler, Rob and Ng, Andrew},
	date = {2009-05-12},
	langid = {english},
	file = {Quigley et al. - ROS an open-source Robot Operating System.pdf:/home/david/snap/zotero-snap/common/Zotero/storage/QJJQSXG2/Quigley et al. - ROS an open-source Robot Operating System.pdf:application/pdf},
}

@article{yin_fast_2018,
	title = {A Fast Orientation Invariant Detector Based on the One-stage Method},
	volume = {232},
	url = {https://www.researchgate.net/figure/Detection-results-of-OIYOLO-using-OBB-and-YOLO-using-HBB-on-UAV-DAHUA-dataset-The-first_fig2_329038189},
	doi = {10.1051/matecconf/201823204036},
	abstract = {We propose an object detection method that predicts the orientation bounding boxes ({OBB}) to estimate objects locations, scales and orientations based on {YOLO} (You Only Look Once), which is one of the top detection algorithms performing well both in accuracy and speed. Horizontal bounding boxes({HBB}), which are not robust to orientation variances, are used in the existing object detection methods to detect targets. The proposed orientation invariant {YOLO} ({OIYOLO}) detector can effectively deal with the bird’s eye viewpoint images where the orientation angles of the objects are arbitrary. In order to estimate the rotated angle of objects, we design a new angle loss function. Therefore, the training of {OIYOLO} forces the network to learn the annotated orientation angle of objects, making {OIYOLO} orientation invariances. The proposed approach that predicts {OBB} can be applied in other detection frameworks. In additional, to evaluate the proposed {OIYOLO} detector, we create an {UAV}-{DAHUA} datasets that annotated with objects locations, scales and orientation angles accurately. Extensive experiments conducted on {UAV}-{DAHUA} and {DOTA} datasets demonstrate that {OIYOLO} achieves state-of-the-art detection performance with high efficiency comparing with the baseline {YOLO} algorithms.},
	pages = {7},
	journaltitle = {{MATEC} Web of Conferences},
	shortjournal = {{MATEC} Web of Conferences},
	author = {Yin, Jun and Pan, Huadong and Su, Hui and Liu, Zhonggeng and Peng, Zhirong},
	date = {2018},
}

@software{hempel_rosa_2021,
	title = {{RoSA}: Cube Detector},
	url = {https://orcid.org/0000-0002-3621-7194},
	version = {1.0.0},
	author = {Hempel, Thorsten},
	date = {2021-12-17},
	doi = {10.5281/zenodo.5788773},
}

@online{diesing_how_2021,
	title = {How {AI} and Machine Vision Impact Vision Robotics},
	url = {https://www.qualitymag.com/articles/96664-how-ai-and-machine-vision-impact-vision-robotics},
	titleaddon = {Quality Magazine},
	author = {Diesing, Genevieve},
	urldate = {2022-02-27},
	date = {2021-09-01},
	langid = {english},
	file = {How AI and Machine Vision Impact Vision Robotics | Quality Magazine:/home/david/snap/zotero-snap/common/Zotero/storage/ZX9SEVVV/96664-how-ai-and-machine-vision-impact-vision-robotics.html:text/html},
}